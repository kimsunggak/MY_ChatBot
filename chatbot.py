import streamlit as st
import openai
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_community.chat_models import ChatOpenAI
from langchain.document_loaders import PyPDFLoader

@st.cache_resource
def load_and_embed_pdfs():
     # PDF íŒŒì¼ ë¬¸ì„œ ë¡œë“œ
    pdf_loader = PyPDFLoader('2024-1í•™ê¸° ìˆ˜ê°•ì‹ ì²­ ë° ê°•ì˜ì‹œê°„í‘œ_20240305.pdf', glob="*.pdf")
    # ë¡œë“œí•œ ë¬¸ì„œ documentsì— ì €ì¥
    documents = pdf_loader.load()
     # í…ìŠ¤íŠ¸ë¥¼ íŠ¹ì • í¬ê¸°ë¡œ ë‚˜ëˆŒ ë•Œ ë¬¸ë§¥ ìœ ì§€ì™€ ì •ë³´ ì†ì‹¤ ë°©ì§€ë¥¼ ìœ„í•´ overlap ì ìš©
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    texts = text_splitter.split_documents(documents)
    # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
    embedding = OpenAIEmbeddings(openai_api_key=st.session_state["OPENAI_API"])
    # ë‚˜ëˆ ì§„ í…ìŠ¤íŠ¸ ë©ì–´ë¦¬ë“¤ì„ ë²¡í„°ë¡œ ë³€í™˜í•œ í›„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
    vectordb = Chroma.from_documents(documents=texts, embedding=embedding)

    return vectordb
def main():
    st.set_page_config(
        page_title = "ìˆ˜ê°•ì‹ ì²­ ì±—ë´‡",
        layout = "wide",
        page_icon = ""
    )
    st.title("ìˆ˜ê°•ì‹ ì²­ ì±—ë´‡")
    st.header("")
    
    with st.sidebar:
        st.session_state["OPENAI_API"] = st.text_input(label="OPENAI APIí‚¤",placeholder="ë‹¹ì‹ ì˜ API KEYë¥¼ ì…ë ¥í•˜ì„¸ìš”",value="",type="password")
        st.markdown("---")
        model = st.radio(label="GPT ëª¨ë¸",options=["gpt-4o","gpt-3.5-turbo"])
        st.markdown("---")
    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role":"assistant","content":"ìˆ˜ê°•ì‹ ì²­ì— ê´€ë ¨í•˜ì—¬ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!"}]
    
    for msg in st.session_state.messages:
        if msg["role"] == "assistant":
            icon = "ğŸ¤–"  # ë´‡ ì•„ì´ì½˜
        else:
            icon = "ğŸ§‘â€ğŸ’»"  # ì‚¬ìš©ì ì•„ì´ì½˜      
        st.markdown(f"{icon} {msg['content']}") # ???
    # chat_input í•¨ìˆ˜ë¡œ ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë¥¼ ì…ë ¥ë°›ìŒ
    if prompt := st.chat_input():
        # OpenAI_APIí‚¤ê°€ ì…ë ¥ë˜ì§€ ì•Šì€ ê²½ìš°
        if not st.session_state["OPENAI_API"]:
            st.info("OpenAI API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!!")
            st.stop()
        # openai í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = openai.OpenAI(api_key=st.session_state["OPENAI_API"])
        # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë©”ì‹œì§€ë¥¼ session_state ë©”ì‹œì§€ì— ì¶”ê°€í•˜ê³  í‘œì‹œ
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.chat_message("user").write(prompt)
    
        vectordb = load_and_embed_pdfs()
        retriever = vectordb.as_retriever()

        qa_chain = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(model_name="gpt-4o", temperature=0, openai_api_key=st.session_state["OPENAI_API"]),
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True)
        
        response = qa_chain.invoke(prompt)
        msg = response['result']
        # ì‘ë‹µ ìƒì„±í›„ ë©”ì‹œì§€ í‘œì‹œ
        st.session_state.messages.append({"role": "assistant", "content": msg})
        st.chat_message("assistant").write(msg)


if __name__ == "__main__":
    main()
